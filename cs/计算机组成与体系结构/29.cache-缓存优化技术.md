# cache-缓存优化技术 #

## 1 缓存优化的度量 ##

前面的**缓存平均访问时间公式提供了三种缓存优化的度量量：命中时间、未命中成本、缺失率**，除此之外，再**增加缓存带宽和功耗两个度量**。

缓存未命中的类型：在计算机体系结构世界中将未命中分为 3 类（3C模型）:

* 强制缺失(Compulsory miss)：又称为冷启动缺失，对 Cache Line 的首次访问导致的缺失
* 容量缺失(Capacity miss)：顾名思义，由容量导致的缺失，即采用全相联结构也会出现的缺失类型
* 冲突缺失(Conflict miss)：由于多块竞争同一组引起的缺失，与容量缺失的差别在于如果采用全相联的结构则不会缺失。

注意，第三种类型的未命中(冲突未命中，conflict miss) 出现在硬件中，因为硬件缓存中对项的放置位置有限制，这是由于所谓的集合关联性(set-associativity) 。**conflict miss不会出现在操作系统页面缓存中，因为这样的缓存总是完全关联的(fully-associative) ，即对页面可以放置的内存位置没有限制**。

多线程和多核也会增加缓存的复杂性，需保证多个处理器中缓存的一致性（Coherency），这也加大了缺失的可能。

> Coherency 参考TLP中相关内容的讨论。

## 2 缩短命中时间：小而简单的一级缓存 & 路预测 ##

小而简单的一级缓存 & 路预测，这两种方式除了能缩短命中时间，通常还能降低功耗。

**小而简单的一级缓存：缩短命中时间，降低功耗**

Cache 的结构会影响命中时间，所以我们可以减小容量大小，减小相联度来降低命中时间，但是缺失率又会上升，需要 tradeoff。

* 块太大的话，里面的无用数据可能就更多，块大小提高是会提高传输时间。

* 块数量增加会增加命中时间，如果是全相联，且 Cache 的容量很大的话，命中时间会很高。另外，Cache 容量越大成本功耗等都会增加。

注意，几种度量量可能存在此消彼长的关系：根据实际测试情况发现，相对于容量来说，块大小对缺失率的影响是先降低后趋于平缓或者是有升高的趋势的，这说明一定程度提升块大小有利于降低缺失率，而块大小大到一定程度的话，对于缺失率的降低并没有什么作用反而会使缺失率上升。而容量的提升是能够改善缺失率的，这也很好理解，容量提升，容纳的数据更多，自然会在一定程度的降低缺失率，但也不是无限制的降低，双倍的容量不一定带来双倍的性能。

* 提高相联度是能够降低缺失率的，它会减少冲突缺失，相联度越高，缺失率越低，但有个度不会低于全相联的情况。但相对于直接相联来说提高相联度也增加了命中时间，会出现收益递减的现象。

**路预测（way prediction）：缩短命中时间，降低功耗**

直接映射命中时间低缺失率高，组关联命中时间高缺失率低。现代cache设计一般采用杂糅方式： n 路组关联。因为组相连缓存中，每一个组里面的N个路（block）是全相连的。也就是相当于读的时候，每次映射好一个set之后，要遍历一遍N个block，当N越大的时候费的时间就越多。

* 所谓路预测（Way prediction），是指在组相联缓存中，跟踪同一组内不同缓存块的使用情况，然后在访问到来时，不经比较直接返回预测的缓存块。当然，标签比较仍然会进行，并且如果发现比较结果不同于预测结果，就会重新送出正确的缓存块。也就是说，错误预测会造成一个缓存块长度的延迟。

> 预测的具体方式是对PC或数据寄存器进行XOR操作，根据数据局部性和代码顺序执行的特点进行预测。参考[Reducing Set-Associative Cache Energy via Way-Prediction and Selective Direct-Mapping](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.7207&rep=rep1&type=pdf) 第2.2.1

路预测（way prediction），它的思想就是在缓存的每个块中添加预测位，来预测在下一次缓存访问时，要访问该组里的哪个块。当下一次访问时，如果预测准了就节省了遍历的时间（相当于直接相连的速度了）；如果不准就再遍历。

模拟表明路预测的准确率超过85%。这种技术非常适合于投机执行（Speculative Execution）处理器，因为这种处理器有完善的机制来保证在投机失败之后取消已经派发的指令。不过有个缺点就是Hit Time不再是确定的几个cycle了（因为没命中的时候要花的cycle多），不便于后面进行优化（参考CPU pipeline）。 

## 3 增加缓存带宽：流水化缓存、多组缓存、无阻塞缓存 ##

**流水化缓存（Pipelined Cache）**

Memory写会需要两个时钟周期：

* 一个周期用来比较标签（Tag）
* 一个周期用来写入读取数据（如果命中）

所以，为了让效率最大化，我们就可以使用流水线缓存（Pipelined Cache）来对这两个时序进行 Pipelining。

为了拆分这两个步骤，设计人员在缓存旁边新添加了一个模块，叫做（缓存延迟存储缓冲区 Delayed Cache Store Buffer）。数据将在里面持续等待，直到被写入缓存。那么数据要等到什么时候呢？答案是当遇到下一个 Store 指令的时候，在其比较标签 Tag 的时候，来自上一个 Store 指令的数据就可以完成第二步：从缓冲区写入缓存。

将一级缓存并入流水线是一般做法。这种做法可行性在于一级缓存的访问时间通常都极短，可能只有一到数个CPU周期。此外，由于TLB也是一种高速缓存硬件，故也可以纳入流水线。

看似 Pipelined Cache 这种优化思路能够拆分写入缓存的步骤，减少写命中时间，但从硬件实现的角度来说，这一点还需要斟酌。假设有一个情况，一个 Load 指令紧接着 Store 指令，那么这个 Load 指令就需要去 Delayed Stored Buffer 里面寻找有没有相同的地址。如果有，就需要使用 Bypass 把数据送还给 CPU。为了实现这一点，一些新的硬件模块比如多路复用器（MUX）和比较器（Comparator）被加入到了缓存系统，而新的硬件则会为 Timing 和命中时间（Hit Time）带来新的挑战。

另外，这一变化增加了流水线的段数，增加了预测错误分支的代价，延长了从发出载入指令到使用数据之间的时钟周期数，但的确更便于采用高相联度的缓存。

**多组缓存**

可以将缓存划分为几个相互独立、支持同时访向的级存组，而不是将它们看作一个整体。分组方式最初用于提高主存储器的性能，现在也用于 DRAM 芯片和缓存中。显然，当访问请求很自然地分布在级存组之间时，分组方式的效果最佳，所以将地址映射到缓存组的方式影响着存储器系统的行为。一种简单有效的映射方式是将缓存块地址按顺序分散在这些级存组中，这种方式称为顺序交错。采用分组方式可以降低缓存功耗。

Arm Cortex-A8在其L2缓存中支持1至4个缓存组；Intel Core 7的L1中有4个组（每个时钟周期内可以支持2次存储器访问），L2有8个组。

**无阻塞缓存**

一般而言，当缓存发生失效时，处理器必须停滞（stall），等待缓存将数据从次级存储中读取出来。对于乱序执行（Out-of-order Execution）处理器，有必要设计这样一种缓存，使之能够在处理缓存失效的同时，继续接受来自处理器的访问请求，这称为非阻塞缓存（Non-blocking cache）。否则，缓存的阻塞在乱序处理器中会阻止LSU（load/store unit）向其发出更多的存储或读取的访问，因此会影响整个处理器的运行速度。

为了解决这样的问题，引入一个叫做Miss Status Holding Registers (MSHRs)的结构来保存未解决的cache miss的信息。拥有MSHRs的缓存结构将可以接受多个cache miss，因此叫做非阻塞缓存（Non-blocking cache）。


注意，对于读缺失，通常会导致阻塞，因为 CPU 想要继续处理，必须取得读的数据。但是写缺失不一样，CPU 不需要写操作的结果，只要写进去就行了，所以一般对于写缺失可以不阻塞，通过write buffer使其在 “后台” 完成写操作，以此来降低写缺失的代价。（write buffer一是可以用于不阻塞CPU写，另一个作用就是合并写操作，具体参考后面讨论）。

write through 虽然能够很好的保证数据一致性，但是慢，因为每次都要写当前一级的存储，还要写下一级的存储。所以 write buffer 就是一个很好的解决方案，一般来说高性能的 CPU 写通模式的 Cache 旁边都要放一个 write buffer，当然不是说只有write through会有，比如说写回策略脏块被替换出去写到下一级存储时也可以交由 write buffer 来做，这看具体设计，tradeoff。

## 4 降低未命中成本：关键字优化&提前重启、合并写缓冲区 ##

**关键字优化Critical Word First & 提前重启Early Restart**

相对一个Word来说，缓存块的大小cache block size一般是比较大的。有时候cpu可能只需要一个block中的某一个word，那么如果cpu还要等整个block传输完才能读这个word就有点慢了。因此我们就有了两种加速的策略：

* Critical Word First：首先从存储器中读想要的word，在它到达cache后就立即发给CPU。然后在载入其他目前不急需的word的同时，CPU继续运行。
* Early Restart：或者就按正常顺序载入一整个block。当所需的word到达cache后就立即发给CPU。然后在载入其他目前不急需的word的同时，CPU继续运行

根据locality的原理，一般来说CPU接下来要访问的也就是这个block中的剩余内容。

**合并写缓冲区**

除了使得 CPU 不阻塞暂停之外，write buffer 还有一个作用，那就是减少写的流量。它可以吸收(absorb) 写操作，当 CPU 对同一个块多次写时，因为 write buffer 的存在吸收掉着多次写操作，使得只对下一级存储进行一次写操作，减少写流量。另外再说明一点，write buffer 的存在并不一定使得 CPU 不暂停阻塞，如果 write buffer 满了的话，CPU 需要等待 write buffer 为空。

write buffer 是一个 FIFO，通常深度(就理解为能够支持的 Cache 块数)不用太多，几项就能够有很好的性能了。每一项需要包括哪些信息呢？数据肯定是必不可少的，再者就是块的地址信息，因为需要知道 buffer 里面的数据写到哪儿去。

> write buffer 的两个主要作用，一是“不阻塞”CPU，让写操作后台完成以此来隐藏写的延迟，二是吸收写操作减少写流量。有意思的是这两个作用对 write buffer 的状态要求不同，buffer 为空对第一个功能有利，而 buffer 满对第二个功能有利。

write buffer 的一个典型的不好问题就是 写缺失后紧接着读缺失。

## 5 降低缺失率：编译器优化 ##

* 改善空间局部性：所谓改善空间局部性就是尽量将近期会接连访问的数据指令放在一起。
* 改善时间局部性：时间局部性是说访问过的数据近期可能再次访问，而改善程序的时间局部性就是把这个 “近期” 缩小，如果能缩小到就是下一次会访问那就再好不过。

**指令重排**

前面说过大多数指令都是顺序执行，有着良好的空间局部性，主要是分支类的指令破坏了局部性，编译器的优化技术可以改善它。解决方案如下：

1. 先简单测试每种分支跳转与不跳转的概率
2. 构建流图
3. 寻找每个指令组后续可能执行的指令组
4. 按照最大可能的执行顺序方式重排指令组

抓住按照最大可能的执行顺序方式重排指令组。

**循环交换**

假设：Cache 结构：容量 40B，块大小 20B，全相联，则一行能放 5个 int 型数据，有 2 行

```
int x[5][5];
/***初始化获取数据 略***/
for(int j = 0; j < 5; j++)
    for(int i = 0; i < 5; i++)
        x[i][j]++;
```

在这个例子当中，每次 Cache 缺失都会直接获取一个块的数据，也就是二维数组的一行 5 个 int 型数据，但是由于这个程序是以列的形式访问，所以拿进来的 5 个数据只有 1 个有用，其他 4 个数据都是无用数据，所以这种程序的空间局部性不好，在一定时间内我们要尽量访问相邻的数据:

```
int x[5][5];
/***初始化获取数据 略***/
for(int i = 0; i < 5; i++)
    for(int j = 0; j < 5; j++)
        x[i][j]++;
```

**分块**

分块(chunk)主要还是对数组来说的，在程序中，我们经常会对数组元素访问，通常如果数组很大，Cache 不能放下整个数组，这时我们可以将数组分块，就跟矩阵分块一样。比如说我一个数组分成了 A B C D，四个块，Cache 的确不能将整个数组放下，但是能一次性放下 chunk A(这里的块和 Cache 里的块重叠，我用英文代替)，我先把所有访问 chunk A 的元素操作完成，这样就避免了访问数组里其他区域元素时替换 chunk A 的数据，改善了时间局部性。

```
/***k次遍历数组x[][]***/
for(int k = 0; k < MAXK; k++)
    for(int i = 0; i < N; i++)
        for(int j = 0; j < N; j++)
            x[i][j]++;


/********分块*********/
for(int ii = 0; ii < N; ii += B)
  for(int jj = 0; jj < N; jj += B)
    for(int k = 0; k < MAXK; k++)
      for(int i = ii; i < min(ii + B, N); i++)
        for(int j = jj; j < min(jj + B, N); j++)
          x[i][j]++;
```

## 6 通过并行降低未命中成本或缺失率：硬件预取或编译器预取 ##

预取（prefetching）——预测未来的内存访问，并在处理器显式访问前对就相应内存块发出请求。——其作为一种隐藏内存访问延迟的方法是非常具有前途的。已经存在了大量用于预期的软硬件方法。许多针对简单存取模式的硬件预取机制已被纳入现代微处理器中，以预取指令与数据。

预取从实现上分类可分为硬件预取和软件预取。

* 硬件预取：在 Cache 旁边放置一个预取器，它来预测哪些主存块在将来会被访问，然后提前预取这些将来可能会被访问的主存块。
* 软件预取：在编写程序时使用一些特殊的编译制导预取语句。

预取机制：

* 顺序预取器，简称为 OBL(One Block Lookahead)，从英文名字上看大概就猜到什么意思了，在 Cache 访问缺失的时候，不仅读取缺失的块，还预取与其相邻的下一块。这种预取器对于顺序流有很好的预取效果，比如说指令流。

* 步幅预取器，观察学习数据访问的步幅规律确定预取的主存块，比如说，当步幅为 n 时，Cache 缺失时取主存块 i 时，预取第 i + n 块，所以顺序预取器其实就是步幅为 1 的步幅预取器。