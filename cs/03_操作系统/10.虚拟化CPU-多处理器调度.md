# 虚拟化CPU-多处理器调度

## 1 背景：缓存一致性、同步、缓存亲和度 ##

在单 CPU 系统中，存在多级的硬件缓存(hardware cache) ，一般来说会让处理器更快地执行程序。缓存是很小但很快的存储设备，通常拥有内存中最热的数据的备份。相比之下，内存很大且拥有所有的数据，但访问速度较慢。通过将频繁访问的数据放在缓存中， 系统似乎拥有又大又快的内存。

缓存是基于局部性(locality)的概念，局部性有两种，即时间局部性和空间局部性。时间局部性是指当一个数据被访问后，它很有可能会在不久的将来被再一访问，比如循环代码中的数据或指令本身。而空间局部性指的是，当程序访问地址为 x 的数据时，很有可能会紧接着访问 x 周围的数据，比如遍历数组或指令的顺序执行。由于这两种局部性存在于大多数的程序中，硬件系统可以很好地预测哪些数据可以放入缓存，从而运行得很好。

多 CPU 的情况下存在**缓存一致性(cache coherence)问题**。硬件提供了这个问题的基本解决方案:通过监控内存访问，硬件可以保证获得正确的数据，并保证共享内存的唯一性。在基于总线的系统中，一种方式是使用总线窥探(bus  snooping)。每个缓存都通过监听链接所有缓存和内存的总线，来发现内存访问。如果 CPU 发现对它放在缓存中的数据的更新， 会作废 (invalidate) 本地副本 (从缓存中移除) ， 或更新(update)它(修改为新值) 。回写缓存，如上面提到的，让事情更复杂。

既然缓存已经做了这么多工作来提供一致性，应用程序(或操作系统)还需要关心共享数据的访问吗?依然需要!**跨 CPU 访问(尤其是写入)共享数据或数据结构时，需要使用互斥原语(比如锁) ，才能保证正确性**(其他方法，如使用无锁(lock-free)数据结构，很复杂，偶尔才使用。详情参见并发部分关于死锁的章节) 。

在设计多处理器调度时遇到的最后一个问题， 是所谓的**缓存亲和度 (cache affinity)** 。 这个概念很简单:一个进程在某个 CPU 上运行时，会在该 CPU 的缓存中维护许多状态。下一该进程在相同 CPU 上运行时，由于缓存中的数据而执行得更快。相反，在不同的 CPU 上执行，会由于需要重新加载数据而很慢(好在硬件保证的缓存一致性可以保证正确执行) 。因此多处理器调度应该考虑到这种缓存亲和性，并尽可能将进程保持在同一个 CPU 上。

## 2 最简单：单队列调度 ##

最基本的方式是简单地复用单处理器调度的基本架构，将所有需要调度的工作放入一个单独的队列中，我们称之为**单队列多处理器调度(Single Queue Multiprocessor Scheduling，SQMS) **。这个方法最大的优点是简单。它不需要太多修改，就可以将原有的策略用于多个 CPU，选择最适合的工作来运行(例如，如果有两个 CPU，它可能选择两个最合适的工作) 。

，SQMS 有几个明显的短板。第一个是缺乏可扩展性(scalability) 。为了保证在多CPU 上正常运行，调度程序的开发者需要在代码中通过加锁(locking)来保证原子性，如上所述。在 SQMS 访问单个队列时(如寻找下一个运行的工作) ，锁确保得到正确的结果。  然而，锁可能带来巨大的性能损失，尤其是间着系统中的 CPU 数增加时。间着这种单个锁的争用增加，系统花费了越来越多的时间在锁的开销上，较少的时间用于系统应该完成的工作。 

SQMS 的第二个主要问题是缓存亲和性。

## 3 多队列调度 ##

正是由于单队列调度程序的这些问题，有些系统使用了多队列的方案，比如每个 CPU 一个队列。我们称之为**多队列多处理器调度(Multi-Queue Multiprocessor Scheduling，MQMS）**。

在 MQMS 中，基本调度我架包含多个调度队列，每个队列可以使用不同的调度规则， 比如轮转或其他任何可能的算法。 当一个工作进入系统后， 系统会依照一些启发性规则 (如间机或选择较空的队列) 将其放入某个调度队列。 这样一来， 每个 CPU 调度之间相互独立， 就避免了单队列的方式中由于数据共享及同步带来的问题。

MQMS 比 SQMS 有明显的优势， 它天生更具有可扩展性。 队列的数量会间着 CPU 的增加而增加，因此锁和缓存争用的开销不是大问题。此外，MQMS 天生具有良好的缓存亲和度。所有工作都保持在固定的 CPU 上，因而可以很好地利用缓存数据。

但是，如果稍加注意，你可能会发现有一个新问题(这在多队列的方法中是根本的) ， 即负载不均(load imbalance) 。最明显的答案是让工作移动，这种技术我们称为迁移(migration) 。通过不断地跨 CPU 迁移一个或多个工作，可以真正实现负载均衡。一个基本的方法是采用一种技术，名为**工作窃取(work stealing)**。通过这种方法， 工作量较少的(源)队列不定期地“偷看”其他(目标)队列是不是比自己的工作多。如果目标队列比源队列(显著地)更满，就从目标队列“窃取”一个或多个工作，实现负载均衡。 当然， 这种方法也有让人抓狂的地方——如果太频繁地检查其他队列， 就会带来较高的开销，可扩展性不好，而这是多队列调度最初的全部目标!相反，如果检查间隔太长，又可能会带来严重的负载不均。找到合适的阈值仍然是黑魔法，这在系统策略设计中很常见。

有趣的是，在构建多处理器调度程序方面，Linux 社区一直没有达成共识。一直以来， 存在 3 种不同的调度程序: O(1)调度程序、 完全公平调度程序 (CFS) 以及 BF 调度程序 (BFS，Brain Fuck Scheduler)。O(1) 和CFS 采用多队列，而 BFS 采用单队列，这说明两种方法都可以成功。当然它们之间还有很多不同的细节。 例如， O(1)调度程序是基于优先级的 (类似于之前介绍的 MLFQ) ， 间时间推移改变进程的优先级，然后调度最高优先级进程，来实现各种调度目标。交互性得到了特别关注。 与之不同， CFS 是确定的比例调度方法 (类似之前介绍的步长调度) 。 BFS 作为三个算法中唯一采用单队列的算法，也基于比例调度，但采用了更复杂的方案，称为最早最合适虚拟截止时间优先算法(EEVEF)
