# Java语言-ConcurrentHashMap

```
//ConcurrentHashMap具有容量8和负载因子0.6
ConcurrentHashMap<Key, Value> numbers = new ConcurrentHashMap<>(8, 0.6f);
```

## 1 HashMap：不支持并发

众所周知 HashMap 底层是基于 数组 + 多个链表 组成的（数组存放的是当前 key 的 hashcode，链表中存的是实际的kv数据），不过在 jdk1.7 和 1.8 中具体实现稍有不同。

注意，无论是 1.7 还是 1.8 其实都能看出 JDK 没有对它做任何的并发操作支持，所以并发会出问题，甚至 1.7 中出现死循环导致系统不可用（1.8 已经修复死循环问题）。

**1.7中的HashMap：数组 + 多个链表**

HashMap给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。

在JDK1.7中，当 Hash 冲突严重时，在桶上形成的链表会变的越来越长，这样在查询时的效率就会越来越低；时间复杂度为 O(N)。因此 1.8 中重点优化了这个查询效率。

**1.8中的HashMap：数组 + 多个链表/红黑树**

1.8的实现中，最初还是按链表插入，但是会判断当前链表的大小是否大于预设的阈值，大于时就要转换为红黑树。修改为红黑树之后查询效率直接提高到了 O(logn)。

## 2 Hashtable：全表锁并发性能不高

HashMap和Hashtable的底层实现都是数组+链表结构实现。

HashMap中未进行同步考虑，而Hashtable则使用了synchronized，我们可以在单线程时使用HashMap提高效率，而多线程时用Hashtable来保证安全。但synchronized是针对整张Hash表的，即每次锁住整张表让线程独占，安全的背后是巨大的浪费。

## 3 ConcurrentHashMap：read不加锁，write细粒度锁

ConcurrentHashMap由 多个Segment 数组 + 多个HashEntry 组成（Segment数组，可以做到读取数据不加锁，并且其内部的结构可以让其在进行写操作的时候能够将锁的粒度保持地尽量地小，允许多个修改操作并发进行，其关键在于使用了锁分段技术。

ConcurrentHashMap使用了多个锁来控制对hash表的不同部分进行的修改。ConcurrentHashMap 同样也分为 1.7 、1.8 版，两者在实现上略有不同。

**1.7中的ConcurrentHashMap：多个Segment 数组 + 多个HashEntry**

对于JDK1.7版本的实现, ConcurrentHashMap内部使用段(Segment)来表示这些不同的部分，每个段其实就是一个小的Hashtable，它们有自己的锁。只要多个修改操作发生在不同的段上，它们就可以并发进行。

**1.8中的ConcurrentHashMap：多个Segment 数组 + 多个HashEntry/红黑树**

JDK1.8的实现降低锁的粒度，JDK1.7版本锁的粒度是基于Segment的，一个segment包含多个HashEntry，而JDK1.8锁的粒度就是HashEntry（首节点）。1.8中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。
