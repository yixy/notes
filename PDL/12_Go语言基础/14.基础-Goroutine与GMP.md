# 基础-Goroutine与GMP #

**Go的runtime本质上是一个语言库，并不是一个虚拟机。**Go有一个很广泛的runtime类库，它是每个Go程序的一部分。runtime库实现了GC、并发、栈管理以及其他关键的Go语言特性。虽然runtime是Go语言中最重要的内容，但是Go的runtime库更类似于C的libc库。实际上，Go的runtime并不像Java 运行时一样包含一个virtual machine。Go 程序会提前编译为本地机器代码（对于某些变体实现，可能是JavaScript 或 WebAssembly）。因此，尽管该术语通常用于描述程序运行的虚拟环境，但在 Go 中，“运行时”一词只是提供关键语言服务的库的名称。

Goroutines 在同一个用户地址空间里并行独立执行 functions，channels 则用于 goroutines 间的通信和同步访问控制。Go的runtime提供goroutine、channel和内存分配的相关支持。

## 0 goroutine

创建时默认的stack：

* JDK5以后的JavaThreadStack默认为1M
* Groutine的stack初始化大小为2k

和KSE（Kernel Space Entity）的对应关系（KSE可以认为是内核线程）：

* Java Thread是1:1
* Groutine是M:N

Go程序从初始化main package并执行main()函数开始，当main()函数返回时，程序退出，且程序并不等待其他goroutine（非主goroutine）结束。

> 一般的，执行一个函数时通常会等待函数的结果返回。但是开启goroutine时，goroutine只管执行，并且是和主goroutine分开执行，并不会等待结果返回，这是基本的构成思想。

**每次在程序中使用 go 关键字启动 goroutine 时，都必须知道该 goroutine 将如何退出以及何时退出。如果你不知道答案，那就是潜在的内存泄漏。**

**内存栈占用方面goroutine占用空间更小**。创建一个 goroutine 的栈内存消耗为 2 KB(Linux AMD64 Go v1.4后)，运行过程中，如果栈空间不够用，会自动进行扩容。而栈空间一般是固定的，且分配空间较大。

> 创建一个 thread 为了尽量避免极端情况下操作系统线程栈的溢出，默认会为其分配一个较大的栈内存( 1 - 8 MB 栈内存，线程标准 POSIX Thread)，而且还需要一个被称为 “guard page” 的区域用于和其他 thread 的栈空间进行隔离。而栈内存空间一旦创建和初始化完成之后其大小就不能再有变化，这决定了在某些特殊场景下系统线程栈还是有溢出的风险。

**goroutine创建/销毁/切换调度不需要陷入内核，成本比 threads 要小得多**。线程创建和销毀都会有巨大的消耗，是内核级的交互(trap)。

> POSIX 线程(定义了创建和操纵线程的一套 API)通常是在已有的进程模型中增加的逻辑扩展，所以线程控制和进程控制很相似。而进入内核调度所消耗的性能代价比较高，开销较大。goroutine 是用户态线程，是由 go runtime 管理，创建和销毁的消耗非常小。

> 抛开陷入内核，线程切换会消耗 1000-1500 纳秒(上下文保存成本高，较多寄存器，公平性，复杂时间计算统计)，一个纳秒平均可以执行 12-18 条指令。所以由于线程切换，执行指令的条数会减少 12000-18000。goroutine 的切换约为 200 ns(用户态、3个寄存器)，相当于 2400-3600 条指令。因此，goroutines 切换成本比 threads 要小得多。

**复杂性方面，goroutine基于消息通信机制更简单**。线程的创建和退出复杂，多个 thread 间通讯复杂(share memory)。

> 不能大量创建线程(参考早期的 httpd)，成本高。使用网络多路复用，存在大量callback(参考twemproxy、nginx 的代码)。对于应用服务线程门槛高，例如需要做第三方库隔离，需要考虑引入线程池等。


## 1 Go1.2之前的GM模式

![](https://raw.githubusercontent.com/yixy4app/images/picgo/202406082135651.png)

* G：Goroutine
* M：Machine（OS thread）：通过M.stack指向G.stack，PC指向G提供的函数，然后去执行goroutine

Go1.2之前的调度器实现是GM模式（由M执行G，待执行的G在全局队列中），存在如下问题：

1. 单一全局队列锁：所有G相关的操作，如创建（放入G至队列）、结束（从队列取G）、重新调度（放G取G）都会有全局队列锁竞争。
2. 每个M持有内存缓存（mcache和stackalloc）：GM模型中，早期设计的每个 M 持有 mcache 和 stackalloc。1）只有在 M 运行 Go 代码时才需要使用该并发内存，当 M 在处于 syscall 时则不需要。运行 Go 代码和阻塞在 syscall 的 M 的比例一般可能高达1:100，这样每个M都持有相关内存就会造成很大的浪费。2）同时内存亲缘性也较差，G 当前在 M 运行后对 M 的内存进行了预热，因为 G 调度到同一个 M 的概率不高，数据局部性不好。
3. 严重的线程阻塞和唤醒：在系统调用的情况下，工作线程经常被阻塞和唤醒，这增加了很多开销。比如 M 找不到G，此时 M 就会进入频繁阻塞（没有G则M阻塞）；或者发现没有M处理G，则唤醒M来进行检查的逻辑，以便及时发现新的 G 来执行。

> 向OS申请内存时（调用malloc）会进行全局加锁。Go会让每个M先申请内存（mcache，每个可以高达2mb)，如果被G的堆对象用完了再由M去申请，通过M来实现内存申请动作加速。

> stackalloc对应栈空间。

## 2 GMP模式

引入P解决上述问题：

* G：Goroutine
* M：Machine（OS thread）：通过M.stack指向G.stack，PC指向G提供的函数，然后去执行goroutine。
* P：Processor：通过Processor结构体来抽象线程的上下文，可以看作处理用户代码逻辑的处理器，但它并不是真的物理CPU。它负责衔接 M 和 G 的调度上下文，将等待执行的 G 与 M 对接。当 P 有任务时需要创建或者唤醒一个 M 来执行它队列里的任务。所以 P/M 需要进行绑定，构成一个执行单元。P 决定了并行任务的数量，可通过 runtime.GOMAXPROCS 来设定。在 Go1.5 之后GOMAXPROCS 被默认设置可用的核数，而之前则默认为1。mcache和stackalloc被迁移到P，同时除全局G队列外，每个P会有一个本地G队列。

![](https://raw.githubusercontent.com/yixy4app/images/picgo/202406082134182.png)

> 早期Docker容器中看到的核心数是宿主机的，很容易触发cgroup的限流，会造成系统延迟抖动。

> P的本地队列还是会有并发访问场景，Go采用Lockfree算法（CAS）实现本地队列。

**通过引入线程自旋，避免在等待可用的P和G时频繁的阻塞和唤醒线程。** Go中自旋线程就是循环执行一个指定逻辑（这里是指调度逻辑，目的是不停的寻找G）。同时当有类型1的自旋M存在时，类型2的自旋M就不阻塞。因为阻塞会释放P，一释放P就马上被类型1的自旋M抢走了，没必要。

* 类型1:M不带P的找P挂载。（一有P释放就结合）
* 类型2:M带P的找G运行（一有runable的G就执行）

### 2.0 GMP模式-Go程序启动

Go程序启动后有一个主线程m0：

1. 创建GOMAXPROCS个P，存储在sched的空闲链表（pidle）。绑定m0和p0。
2. 创建一个指向runtime.main函数（启动sysomn线程；启动GC协程；执行init；执行main.main函数）的G，并放到p0本地队列。
3. m0会执行一个特殊的G：g0（g0负责调度，即schedule函数）。此时，之前创建的指向runtime.main函数的G将被调度运行。

schedule（g0）的逻辑如下：

1. 判断是否符合条件（每隔60轮，1/61的概率），符合则去全局队列拿一个G，这样是为了避免全局队列饥饿。
2. 先看本地队列有没有待执行的G，有则取本地队列执行。
3. 如果本地队列没有则去全局队列里拿(全局队列当前个数/GOMAXPROCS)个G执行。
4. 当一个 P 执行完本地所有的 G 之后，并且全局队列为空的时候，会尝试work-stealing，即挑选一个受害者P，从它的 G 队列中窃取一半的G。为了保证公平性，work-stealing从随机位置上的 P 开始，而且遍历的顺序也随机化了(选择一个小于 GOMAXPROCS，且为质数的步长)，保证遍历的顺序也随机化。
5. 如果work-stealing也拿不到G，则会去看是否有netpoll处理。

为了避免浪费CPU资源，自旋的M也最多只允许存在GOMAXPROCS个。 **在创建新G、M系统调用阻塞、自旋M（执行g0）进入活跃状态（执行G）等三个场景会进行检查，如果存在空闲P，则尝试新建或唤醒M进入自旋（自旋会执行一个特殊的G，即g0，它负责管理和调度G）。**

g0基于两种断点对G进行重新调度（调度到线程上）：

* 当G阻塞时：系统调用、互斥锁或 chan。阻塞的G进入睡眠模式进入队列，并允许Go 安排和运行等待其他的G。
* 在函数调用期间，如果G必须扩展其堆栈。这个断点允许 Go 调度另一个G 并避免运行G 占用 CPU。

在这两种情况下，运行调度程序的g0 将当前G 替换为另一个G，即ready to run。然后，选择的G 替换g0 并在线程上运行。与常规G相反，g0 有一个固定和更大的栈（stackalloc）。g0还负责Defer 函数的分配，GC收集（比如STW、扫描G 的堆栈和标记、清除操作）、栈扩容（当露要的时候，由g0 进行扩栈操作）。

### 2.1 GMP模式-新建G

* 新建 G 时 P 的本地 G 队列放不下（达到256个的时候）会放半数 G 到全局队列去。

### 2.2 GMP模式-G执行完成

* G执行完成时会执行g0的schedule逻辑。

### 2.3 GMP模式-G调用系统调用

* 调用syscall后M会解绑P，M和G进入阻塞，P的状态是syscall（表明这个P的G正在syscall中，这时P是不能调度给别的M的）。如果短时间内阻塞的M被唤醒，那么M就会优先重新获取这个P并重新绑定恢复执行。
* 系统监视器 (Syster monitor)，称为 sysmon，会进行定时扫描。在执行 syscall 时，如果某个P的G执行超过一个 sysmon tick(10ms)，就会把这个P设为idlle强制解绑并放入P的idlelist，新建或重新调度给需要的 M。
* 阻塞的系统调用返回时M会尝试获取之前的P，如果获取不到就会去P的idlelist找空闲 P，绑定P后恢复G执行，如果没有空闲P则会将G放到全局队列，并将M放到M的idlelist。

**GOMAXPROCS限制的只是P的并行度（The GOMAXPROCS variable limits the number of operating system threads that can execute user-level Go code simultaneously），无法限制线程总数。**

Tips: 当使用了 Syscall， Go 无法限制 Blocked Os threads 的数量，使用 syscall 写程序要认真考虑 pthread exhaust 问题。

> The GOMAXPROCS variable limits the number of operating system threads that can execute user-level Go code simultaneously. There is no limit to the number of threads that can be blocked in system calls on behalf of Go code, those do not count against the GOMAXPROCS limit. This package's GOMAXPROCS function queries and changes the limit.

### 2.4 GMP模式-G的其他阻塞block（gopark）

* gopark函数：将G置为waiting状态，等待显式goready唤醒，如poller、锁、channel进入等待其实都是调用gopark。

> G切换的亲缘性优化：G频繁地切换可能带来P本地队列的频繁排队，由于本地队列时FIFO，如果另一个G占用线程，导致unblock G无法尽快运行，可能会被窃取走导致亲缘性变差。Go1.5针对这个情况进行了亲缘性优化，通过在P引入一个runnext的特殊字段，可以高优先级执行unblock G。

### 2.5 GMP模式-系统监视器（system monitor，即sysmon）

sysmon无需P绑定运行，本身是一个死循环，每20us~10ms循环一次，每次循环完成后sleep一段时间（避免空转，如果当次循环没有处理任何事情，则sleep时间会增大）。

1. 释放闲置超过5分钟的span物理内存。
2. 如果超过2min没有进行GC，则强制执行。（GC也会去拿netpoll）
3. 将长时间未处理的netpoll添加到全局队列。
4. 向长时间运行的G发出抢占调度。
5. 收回因syscall长时间阻塞的P。

* 抢占调度逻辑：当P在M上执行时间超过10ms，sysmon调用preemptone将G标记为stackPreempt。早期Go在检查栈释放溢出的地方判定（进行函数调用时的morestack方法），M会保存当前G的上下文，重新进入调度逻辑（早期实现存在的死循环问题：issues/11462）。go1.14基于信号的抢占式调度实现，原理为：异步抢占，注册 sigurg 信号，通过 sysmon 检测，对M 对应的线程发送信号，触发注册的 handler，它往当前 G 的PC 中插入一条指令(调用某个方法)，在处理完 handler，G 恢复后，自己把自己推到了 global queue中。

* Network poller：Go编程模型里所有IO都是阻塞式的，通过goroutine+channel来处理并发。G发起网络IO操作不会导致M阻塞（仅阻塞G，此时G会进入gopark函数，将正在执行的G状态保存起来，然后切换到新的堆栈上执行新的G），从而不会导致大量M被创建。将异步IO转换为阻塞IO的部分称为netpoller。

> Go调度的缺陷：network poller优先级相对较低，别的调度或者GC可能会导致network poller延迟高。
