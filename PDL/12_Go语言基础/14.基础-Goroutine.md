# 基础-Goroutine #

# 1. Go的runtime #

**Go的runtime本质上是一个语言库，并不是一个虚拟机。**Go有一个很广泛的runtime类库，它是每个Go程序的一部分。runtime库实现了GC、并发、栈管理以及其他关键的Go语言特性。虽然runtime是Go语言中最重要的内容，但是Go的runtime库更类似于C的libc库。实际上，Go的runtime并不像Java 运行时一样包含一个virtual machine。Go 程序会提前编译为本地机器代码（对于某些变体实现，可能是JavaScript 或 WebAssembly）。因此，尽管该术语通常用于描述程序运行的虚拟环境，但在 Go 中，“运行时”一词只是提供关键语言服务的库的名称。

Goroutines 在同一个用户地址空间里并行独立执行 functions，channels 则用于 goroutines 间的通信和同步访问控制。Go的runtime提供goroutine、channel和内存分配的相关支持。

## 2 goroutine与thread ##

创建时默认的stack：

* JDK5以后的JavaThreadStack默认为1M
* Groutine的stack初始化大小为2k

和KSE（Kernel Space Entity）的对应关系（KSE可以认为是内核线程）：

* Java Thread是1:1
* Groutine是M:N

Go程序从初始化main package并执行main()函数开始，当main()函数返回时，程序退出，且程序并不等待其他goroutine（非主goroutine）结束。

> 一般的，执行一个函数时通常会等待函数的结果返回。但是开启goroutine时，goroutine只管执行，并且是和主goroutine分开执行，并不会等待结果返回，这是基本的构成思想。

**每次在程序中使用 go 关键字启动 goroutine 时，都必须知道该 goroutine 将如何退出以及何时退出。如果你不知道答案，那就是潜在的内存泄漏。**

**内存栈占用方面goroutine占用空间更小**。创建一个 goroutine 的栈内存消耗为 2 KB(Linux AMD64 Go v1.4后)，运行过程中，如果栈空间不够用，会自动进行扩容。而栈空间一般是固定的，且分配空间较大。

> 创建一个 thread 为了尽量避免极端情况下操作系统线程栈的溢出，默认会为其分配一个较大的栈内存( 1 - 8 MB 栈内存，线程标准 POSIX Thread)，而且还需要一个被称为 “guard page” 的区域用于和其他 thread 的栈空间进行隔离。而栈内存空间一旦创建和初始化完成之后其大小就不能再有变化，这决定了在某些特殊场景下系统线程栈还是有溢出的风险。

**goroutine创建/销毁/切换调度不需要陷入内核，成本比 threads 要小得多**。线程创建和销毀都会有巨大的消耗，是内核级的交互(trap)。

> POSIX 线程(定义了创建和操纵线程的一套 API)通常是在已有的进程模型中增加的逻辑扩展，所以线程控制和进程控制很相似。而进入内核调度所消耗的性能代价比较高，开销较大。goroutine 是用户态线程，是由 go runtime 管理，创建和销毁的消耗非常小。

> 抛开陷入内核，线程切换会消耗 1000-1500 纳秒(上下文保存成本高，较多寄存器，公平性，复杂时间计算统计)，一个纳秒平均可以执行 12-18 条指令。所以由于线程切换，执行指令的条数会减少 12000-18000。goroutine 的切换约为 200 ns(用户态、3个寄存器)，相当于 2400-3600 条指令。因此，goroutines 切换成本比 threads 要小得多。

**复杂性方面，goroutine基于消息通信机制更简单**。线程的创建和退出复杂，多个 thread 间通讯复杂(share memory)。

> 不能大量创建线程(参考早期的 httpd)，成本高。使用网络多路复用，存在大量callback(参考twemproxy、nginx 的代码)。对于应用服务线程门槛高，例如需要做第三方库隔离，需要考虑引入线程池等。

## 3 goroutine的M:N模型 ##

Go 创建 M 个线程(CPU 执行调度的单元，内核的 task_struct)，之后创建的 N 个 goroutine 都会依附在这 M 个线程上执行，即 M:N 模型。goroutine能够同时运行，与线程类似，但相比之下非常轻量。因此，程序运行时，Goroutines 的个数应该是远大于线程的个数的（pthread 是内核线程）。同一个时刻，一个线程只能跑一个 goroutine。当 goroutine 发生阻塞 (chan 阻塞、mutex、syscall 等等) 时，Go 会把当前的 goroutine 调度走，让其他 goroutine 来继续执行，而不是让线程阻塞休眠，尽可能多的分发任务出去，让 CPU 忙。

## 4 GMP调度原理 ##

* G：goroutine 的缩写，每次 go func() 都代表一个 G，无限制。使用 struct runtime.g结构，包含了当前 goroutine 的状态、堆栈、上下文。
* M：工作线程(OS thread)也被称为 Machine，使用 struct runtime.m结构，所有 M 是有线程栈的。如果不对该线程栈提供内存的话，系统会给该线程栈提供内存(不同操作系统提供的线程栈大小不同)。当指定了线程栈，则 M.stack指向G.stack，M 的 PC 寄存器指向 G 提供的函数，然后去执行。

Go 1.2前的调度器实现为GM模型，该模型限制了 Go 并发程序的伸缩性，尤其是对那些有高吞吐或并行计算需求的服务程序。

![](https://raw.githubusercontent.com/yixy4app/images/picgo/202406082135651.png)

**GM模型**：每个 goroutine 对应于 runtime 中的一个抽象结构：G，而 thread 作为“物理 CPU”的存在而被抽象为一个结构：M(machine)。当 goroutine 调用了一个阻塞的系统调用，运行这个 goroutine 的线程就会被阻塞，这时至少应该再创建/唤醒一个线程来运行别的没有阻塞的 goroutine。线程这里可以创建不止一个，可以按需不断地创建，而活跃的线程（处于非阻塞状态的线程）的最大个数存储在变量 GOMAXPROCS中。

GM调度模型存在以下问题。

**单一全局互斥锁(Sched.Lock)和集中状态存储**：导致所有 goroutine 相关操作，比如创建、结束、重新调度等都要上全局锁。

**Goroutine 传递问题**：经常会在 M 之间传递”可运行”的 goroutine，需要来回切换上锁，这导致调度延迟增大以及额外的性能损耗（刚创建的 G 放到了全局队列，而不是本地 M 执行，不必要的开销和延迟）。

**Per-M 持有内存缓存 (M.mcache)**：GM模型中，早期设计的每个 M 持有 mcache 和 stackalloc，然而只有在 M 运行 Go 代码时才需要使用的内存(每个 mcache 可以高达2mb)，当 M 在处于 syscall 时并不需要。运行 Go 代码和阻塞在 syscall 的 M 的比例一般可能高达1:100，造成了很大的浪费。同时内存亲缘性也较差，G 当前在 M 运行后对 M 的内存进行了预热，因为现在 G 调度到同一个 M 的概率不高，数据局部性不好。

**严重的线程阻塞/解锁**：在系统调用的情况下，工作线程经常被阻塞和取消阻塞，这增加了很多开销。比如 M 找不到G，此时 M 就会进入频繁阻塞（没有G则M阻塞）/唤醒（发现没有M处理G，则唤醒M）来进行检查的逻辑，以便及时发现新的 G 来执行。

基于以上问题，Go后面版本的调度采用了GMP模型。

* P：逻辑处理器“Processor”是一个抽象的概念，并不是真正的物理 CPU。

![](https://raw.githubusercontent.com/yixy4app/images/picgo/202406082134182.png)
s
**GMP调度模型**方案是引入一个结构 P，它代表了 M 所需的上下文环境，也是处理用户级代码逻辑的处理器。它负责衔接 M 和 G 的调度上下文，将等待执行的 G 与 M 对接。当 P 有任务时需要创建或者唤醒一个 M 来执行它队列里的任务。所以 P/M 需要进行绑定，构成一个执行单元。P 决定了并行任务的数量，可通过 runtime.GOMAXPROCS 来设定。在 Go1.5 之后GOMAXPROCS 被默认设置可用的核数，而之前则默认为1。

> Automatically set GOMAXPROCS to match Linux container CPU quota. Tips: https://github.com/uber-go/automaxprocs

**mcache/stackalloc 从 M 移到了 P，而 G 队列也被分成两类，保留全局 G 队列，同时每个 P 中都会有一个本地的 G 队列。**引入了 local queue，因为 P 的存在，runtime 并不需要做一个集中式的 goroutine 调度，每一个 M 都会依次在 P's local queue、global queue 或者其他 P 队列中找 G 执行，减少全局锁对性能的影响。

> 这也是 GMP Work-stealing 调度算法的核心。注意 P 的本地 G 队列还是可能面临一个并发访问的场景，为了避免加锁，这里 P 的本地队列是一个 LockFree的队列，窃取 G 时使用 CAS 原子操作来完成。关于LockFree 和 CAS 的知识参见 Lock-Free。

Work-stealing的过程：当一个 P 执行完本地所有的 G 之后，并且全局队列为空的时候，会尝试挑选一个受害者 P，从它的 G 队列中窃取一半的 G。否则会从全局队列中获取(当前个数/GOMAXPROCS)个 G。为了保证公平性，从随机位置上的 P 开始，而且遍历的顺序也随机化了(选择一个小于 GOMAXPROCS，且和它互为质数的步长)，保证遍历的顺序也随机化了。光窃取失败时获取是不够的，可能会导致全局队列饥饿。P 的调度算法中还会每个 N 轮调度之后就去全局队列拿一个 G。新建 G 时 P 的本地 G 队列放不下已满并达到256个的时候会放半数 G 到全局队列去，阻塞的系统调用返回时找不到空闲 P 也会放到全局队列。