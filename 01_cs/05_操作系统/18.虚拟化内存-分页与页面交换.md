# 虚拟化内存-机制:分页

将空间分割成不同长度的分片，就像虚拟内存管理中的分段。遗憾的是，如空闲空间管理中讨论的一样，这个解决方法存在固有的问题。具体来说，将空间切成不同长度的分片以后，空间本身会**碎片化(fragmented) ， 随着时间推移，分配内存会变得比较透难。因此，值得考虑第二种方法:将空间分割成固定长度的分片。**

另外，**分段还是不足以支持更一般化的稀疏地址空间。**例如，如果有一个很大但是稀疏的堆，都在一个逻辑段中，整个堆仍然必须完整地加载到内存中。换言之，如果使用地址空间的方式不能很好地匹配底层分段的设计目标，分段就不能很好地工作。因此我们需要找到新的解决方案。

## 1 分页原理 ##

**页表是一个函数，它的参数是虚拟页面号（VPN），结果是page frame。**

分页不是将一个进程的地址空间分割成几个不同长度的逻辑段(即代码、堆、段) ，而是分割成固定大小的单元，每个单元称为一页。相应地，我们把物理内存看成是定长槽块的阵列，叫作**页帧(page frame) **。每个这样的页帧包含一个虚拟内存页。

为了记录地址空间的每个虚拟页放在物理内存中的位置，操作系统通常为每个进程保存一个数据结构，称为**页表(page table)** 。页表的主要作用是为地址空间的每个虚拟页面保存地址转换 (address translation) ， 从而让我们知道每个页在物理内存中的位置。页表可能有的条目: (虚拟页 0→物理帧 3) 、 (VP 1→PF 7) 、(VP 2→PF 5)和(VP 3→PF 2) 。

重要的是要记住，这个页表是每个进程的数据结构(我们讨论的大多数页表结构都是每进程的数据结构，我们将接触的一个例外是倒排页表，inverted page table) 。如果在上面的示例中运行另一个进程，操作系统将不得不为它管理不同的页表，因为它的虚拟页显然映射到不同的物理页面(除了共享之外) 。

为了对虚拟地址进行转换，我们必须首先将它分成两个组件:**虚拟页面号(virtual page number，VPN)**和**页内的偏移量(offset) **。通过页表实现虚拟地址到物理地址的转换。

![](https://raw.githubusercontent.com/yixy4app/images/picgo/202209102129178.png)

页表就是一种数据结构，用于将虚拟地址(或者实际上， 是虚拟页号)映射到物理地址(物理帧号) 。因此，任何数据结构都可以采用。最简单的形式称为线性页表(linear page table) ，就是一个数组。操作系统通过虚拟页号(VPN)检索该数组，并在该索引处查找页表项(PTE) ，以便找到期望的物理帧号(PFN) 。页表项（PTE）结构：

* 有效位 (valid bit) 通常用于指示特定地址转换是否有效。所有未使用的空间都被标记位无效，如果进程尝试访问这种内存，就会陷入操作系统，可能导致进程终止。
* 保护位(protection bit) ，表明页是否可以读取、写入或执行。
* 存在位(present bit)表示该页是在物理存储器还是在磁盘上(即它已被换出，swapped out) 。
* 脏位(dirty bit) 也很常见，表明页面被带入内存后是否被修改过。 
* 参考位(reference bit，也被称为访问位，accessed bit)有时用于追踪页是否被访问，也用于确定哪些页很受欢迎， 因此应该保留在内存中。
* PFN：物理页帧号。

![](https://raw.githubusercontent.com/yixy4app/images/picgo/202209102129274.png)

## 2 分页存在的两个问题：访问速度慢&占用空间大  ##

在分页式系统中，有两个问题需要考虑：虚拟地址到物理地址的映射必须非常快；需要考虑应对虚拟地址空间很大的情况。

###### 占用空间大 ######

想象一个典型的 32 位地址空间，带有 4KB 的页。这个虚拟地址分成 20 位的 VPN 和 12 位的偏移量。一个 20 位的 VPN 意味着， 操作系统必须为每个进程管理 2^20个地址转换 (大约一百万) 。假设每个页表格条目(PTE)需要 4 个字节，来保存物理地址转换和任何其他有用的东西， 每个页表就需要巨大的 4MB 内存!这非常大。现在想象一下有 100 个进程在运行:这意味着操作系统会需要 400MB 内存，只是为了所有这些地址转换!

由于页表如此之大，我们没有在 MMU 中利用任何特殊的片上硬件，来存储当前正在运行的进程的页表，而是将每个进程的页表存储在内存中。现在让我们假设页表存在于操作系统管理的物理内存中，稍后我们会看到，很多操作系统内存本身都可以虚拟化，因此页表可以存储在操作系统的虚拟内存中(甚至可以交换到磁盘上)

###### 访问速度慢 ######

硬件必须知道当前正在运行的进程的页表的位置。现在让我们假设一个页表基址寄存器(page-table base register)包含页表的起始位置的物理地址。为了找到想要的 PTE 的位置，硬件将执行以下功能:  

```
VPN     = (VirtualAddress & VPN_MASK) >> SHIFT  
PTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE))
//第1此内存访问
PTE = AccessMemory(PTEAddr)

offset   = VirtualAddress & OFFSET_MASK
PhysAddr = (PTE.PFN << PFN_SHIFT) | offset
//第2此内存访问
Register = AccessMemory(PhysAddr)
```

对于每个内存引用(无论是取指令还是显式加载或存储) ，分页都需要我们执行一个额外的内存引用，以便首先从页表中获取地址转换。工作量很大!额外的内存引用开销很大，在这种情况下，可能会使进程减慢两倍或更多。

## 3 加速访问：TLB ##

对每次内存访问，硬件先检查 TLB，看看其中是否有期望的转换映射，如果有，就完成转换(很快) ，不用访问页表(其中有全部的转换映射) 。

* TLB（Translation Lookaside Buffer，地址转换旁路缓冲存储器）：“大多数程序总是对少量的页面进行多次访问”，基于这样的思想，在MMU中设置一个小型的硬件设备，它包含少量的表项，该设备就是TLB。地址映射首先在TLB中由硬件进行转换，如果TLB中匹配不到才会到去访问内存表项。TLB本质上就是频繁发生的虚拟到物理地址转换的硬件缓存(cache) 。

###### TLB由硬件实现还是软件实现？ ######

注意，最初，（复杂指令集计算机）对TLB的管理和TLB失效处理都完全由MMU硬件来实现，只有在内存中没有找到某个页面时（缺页），才会陷入到操作系统中。后来，很多机器（精简指令集计算机）的几乎所有的页面管理都在软件中实现了。这些机器上，TLB表项被显示地装载。当发生TLB访问失效的时候不再是MMU到页表中查找并取出数据，而是生成一个TLB失效并将问题提交给操作系统解决。事实证明，如果TLB大（比如64个表项）到可以减少失效率，TLB的软件管理也能够变得足够有效。

###### 硬件算法的大体流程 ######

首先从虚拟地址中提取页号(VPN) ， 然后检查 TLB 是否有该 VPN 的转换映射。 如果有， 我们有了 TLB 命中 (TLB hit) ， 这意味着 TLB 有该页的转换映射。 成功! 接下来我们就可以从相关的 TLB 项中取出页帧号(PFN) ，与原来虚拟地址中的偏移量组合形成期望的物理地址(PA) ，并访问内存，假定保护检查没有失败。

如果 CPU 没有在 TLB 中找到转换映射(TLB 未命中) ，我们有一些工作要做。在本例中，硬件访问页表来寻找转换映射，并用该转换映射更新 TLB， 假设该虚拟地址有效，而且我们有相关的访问权限。上述系列操作开销较大，主要是因为访问页表需要额外的内存引用 。最后，当 TLB 更新成功后，系统会重新尝试该指令，这时 TLB 中有了这个转换映射，内存引用得到很快处理。

###### 硬件 TLB 中的内容 ######

一条 TLB 项内容可能像这样: VPN | PFN | 其他位

VPN 和 PFN 同时存在于 TLB 中，因为一条地址映射可能出现在任意位置(用硬件的术语， TLB 被称为全相联的 (fully-associative) 缓存) 。 硬件并行地查找这些项， 看看是否有匹配。

###### 上下文切换时对 TLB 的处理 ######

TLB 中包含的虚拟到物理的地址映射只对当前进程有效，对其他进程是没有意义的。

一种方法是在上下文切换时，简单地清空(flush)TLB， 这样在新进程运行前 TLB 就变成了空的。如果是软件管理 TLB 的系统，可以在发生上下文切换时，通过一条显式(特权)指令来完成。如果是硬件管理 TLB，则可以在页表基址寄存器内容发生变化时清空 TLB(注意，在上下文切换时，操作系统必须改变页表基址寄存器(PTBR) 的值) 。不论哪种情况，清空操作都是把全部有效位(valid)置为 0，本质上清空了 TLB。上下文切换的时候清空 TLB，这是一个可行的解决方案，进程不会再读到错误的地址映射。但是，有一定开销:每次进程运行，当它访问数据和代码页时，都会触发 TLB 未命中。如果操作系统频繁地切换进程，这种开销会很高。

为了减少这种开销，一些系统增加了硬件支持，实现跨上下文切换的 TLB 共享。比如有的系统在 TLB 中添加了一个地址空间标识符(Address Space Identifier，ASID) 。可以把ASID 看作是进程标识符(Process Identifier，PID) ，但通常比 PID 位数少(PID 一般 32 位， ASID 一般是 8 位) 。 如果仍以上面的 TLB 为例， 加上 ASID， 很清楚不同进程可以共享 TLB 了: 只要 ASID 字段来区分原来无法区分的地址映射。

###### TLB 替换策略 ######

TLB 和其他缓存一样，还有一个问题要考虑，即缓存替换(cache replacement) 。

## 4 处理巨大的虚拟地址空间：多级页表与倒排页表 ##

我们现在来解决分页引入的第原个问题:页表太大，因此消耗的内存太多。

###### 简单方法：使用更大的页 ######

简单的解决方案是使用更大的页。然而， 这种方法的主要问题在于， 大内存页会导致每页内的浪费， 这被称为内部碎片 (internal fragmentation)问题(因为浪费在分配单元内部) 。因此，结果是应用程序会分配页，但只用每页的一小部分，而内存很快就会充满这些过大的页。因此，大多数系统在常见的情况下使用相对较小的页大小:4KB(如 x86)或 8KB(如 SPARCv9) 。问题不会如此简单地解决。（实际上，各种实现使用更大的页面大小的主要原因并不是为了节省页表空间。这是为了减少 TLB 的压力，让程序能够访问更多的地址空间而不会遭受太多的 TLB 未命中之苦。）

###### 混合方法:分页和分段 ######

杂合方法不是为进程的整个地址空间提供单个页表，而是为每个逻辑分段提供一个。我们可能有 3 个页表，地址空间的代码、堆和栈部分各有一个。

在分段中，有一个基址(base)寄存器，告诉我们每个段在物理内存中的位置，还有一个界限(bound)或限制(limit)寄存器，告诉我们该段的大小。在杂合方案中， 我们仍然在 MMU 中拥有这些结构。在这里，我们使用基址不是指向段本身，而是保存该段的页表的物理地址。界限寄存器用于指示页表的结尾(即它有多少有效页) 。

以这种方式，与线性页表相比，杂合方法实现了显著的内存节省。栈和堆之间未分配的页不再占用页表中的空间(仅将其标记为无效) 。

程序按逻辑块分段，段内再分页，内存仍以页为信息传送单位，这样的虚拟存储器被称为段页式虚拟存储器。在段页式虚拟存储器中，每个程序对应一个段表，每段对应一个页表，段的长度必须是页长的整数倍，段的起点必须是某一页的起点。

此时，虚拟地址由三部分组成（段号+段内页号+页内偏移量）。CPU根据虚拟地址访问时，首先根据段号得到段表地址；然后从段表中取出该段的页表起始地址，与虚拟地址中的段内页号合成，得到页表地址；最后从页表中取出实际页框号，与页内偏移量拼接形成实际地址。

这种方法并非没有问题。首先，它仍然要求使用分段。正如我们讨论的那样，分段并不像我们需要的那样灵活，因为它假定地址空间有一定的使用模式。例如，如果有一个大而稀疏的堆，仍然可能导致大量的页表浪费。其次，这种杂合导致外部碎片再次出现。尽管大部分内存是以页面大小单位管理的，但页表现在可以是任意大小(是 PTE 的倍数) 。因此，在内存中为它们寻找自由空间更为复杂。出于这些原因，人们继续寻找更好的方式来实现更小的页表。

###### 多级页表 ######

不依赖于分段解决相同的问题:如何去掉页表中的所有无效区域，而不是将它们全部保留在内存中?我们将这种方法称为多级页表(multi-level page table) ，因为它将线性页表变成了类似树的东西。这种方法非常有效，许多现代系统都用它

多级页表的基本思想很简单。首先，将页表分成页大小的单元。然后，如果整页的页表项 (PTE) 无效， 就完全不分配该页的页表。 为了追踪页表的页是否有效 (以及如果有效， 它在内存中的位置) ，使用了名为**页目录(page directory)**的新结构。页目录因此可以告诉你页表的页在哪里，或者页表的整个页不包含有效页。

![](https://raw.githubusercontent.com/yixy4app/images/picgo/202209102130306.png)

采用“多级页表”的原因是避免把全部页表一直保存在内存中，那些从不需要的页表项将被存储在磁盘。一般，页表级数不超过4级。

下面总结下分页、TLB、段页混合和二级页表的结构如下：

![](https://raw.githubusercontent.com/yixy4app/images/picgo/202209102130887.png)

**注意，PD和PT均为线性结构，即PDE（Page Dir Entity）和PTE（Page Table Entity）的地址都需要通过偏移量来线性查找（通过base+index方式）。所以，在多级分页结构中，页目录和页表都是连续存储的，单个页目录或页表大小不得超过系统最大连续可分配单元，这个单元就是单个页面的大小。**

通常，每页的大小一般为4KiB。因为对于32位操作系统来说，4KiB是一个比较合适的值，这样offset占12位（2^2 KiB)，剩下的20位刚好各分一半给页目录和页表（PDE和PTE大小为4B，所以10位刚好占4KiB=2^10*4B）。对于64位的x86系统来说，基于向旧兼容的思想，也可以采用4KiB，这个时候系统只采用48进行寻址，其中offset为12位，剩余的36位给页目录和页表（9+9+9+9，共4级，其中PDE和PTE为8B）。

>Linux与Windows的分段机制原理上类似，都是扁平式的，段基址为0，也就是说CS，SS这些寄存器全部都是0，直接把整个虚拟内存看成一整个“段”。所以简单来说，它们并不想使用这个从16位系统遗留下来的分段机制，而CPU为了保持兼容性还保留了这些分段机制，所以现代OS大都使用这种扁平式的分段管理，将CPU「糊弄」过去。
> Linux 因为分页策略加上与内存相关的异常就已经足够完成内存管理功能。没有必要增加复杂度再引入分段机制。而且分段是X86 CPU提供的功能，而其它CPU家族不一定有，但分页是家家都有。为考虑可移植性，也只能采用分页策略。

###### 反向页表 ######

当虚拟地址空间是2^64字节时（64位计算机），假设页面大小是4KB，需要有2^52个表项。即使采用了多级页表，磁盘中存储的表项数据所占用的空间也是巨额的。此时，解决方案之一是使用倒排页表（invertered page table）。

在这种设计之中，实际内存里每一个页帧有一个表项，而不是每一个虚拟页面有一个表项。在反向页表(inverted page table)中，可以看到页表世界中更极端的空间节省。在这里， 我们保留了一个页表，其中的项代表系统的每个物理页，而不是有许多页表(系统的每个进程一个) 。 页表项告诉我们哪个进程正在使用此页， 以及该进程的哪个虚拟页映射到此物理页。  现在，要找到正确的项，就是要搜索这个数据结构。

采用倒排页表能够显著减少页表项，但是它使得从虚拟地址到物理地址的转换变得更困难，每次必须遍历整个页表来查找，而不是使用虚拟页号做索引。使用TLB能够缓解这个问题。但是当TLB失效时，还需要进行优化，此时可以使用虚拟地址来散列（确保不同散列值的数量与页表项数量相同，相同的散列值的表项使用链表相链）。

## 5 交换空间与页错误(page fault)  ##

前面的讨论我们都假设每个正在运行的进程的地址空间都能放入内存。现在我们将放松这些大的假设，并假设我们需要支持许多同时运行的巨大地址空间。

在硬盘上开辟一部分空间用于物理页的移入和移出。在操作系统中，一般这样的空间称为**交换空间(swap space) **，因为我们将内存中的页交换到其中，并在需要的时候又交换回去。因此，我们会假设操作系统能够以页大小为单元读取或者写入交换空间。 为了达到这个目的， 操作系统需要记住给定页的硬盘地址 (disk address) 。交换空间的大小是非常重要的，它决定了系统在某一时刻能够使用的最大内存页数。

当在 PTE 中查找时，可能发现页不在物理内存中。硬件(或操作系统，在软件管理 TLB 时)判断是否在内存中的方法，是通过页表项中的存在位(present bit) 。如果存在位设置为 1，则表示该页存在于物理内存中，如果存在位设置为零，则页不在内存中，而在硬盘上。访问不在物理内存中的页，这种行为通常被称为**页错误(page fault) **。

操作系统可以用 PTE 中的某些位来存储硬盘地址， 这些位通常用来存储像页的 PFN 这样的数据。当操作系统接收到页错误时，它会在 PTE 中查找地址，并将请求发送到硬盘，将页读取到内存中。

为了保证有少量的空闲内存，大多数操作系统会设置高水位线(High Watermark，HW) 和低水位线(Low Watermark，LW) ，来帮助决定何时从内存中清除页。原理是这样:当操作系统发现有少于 LW 个页可用时，后台负责释放内存的线程会开始运行，直到有 HW 个可用的物理页。这个后台线程有时称为交换守护进程(swap daemon)或页守护进程(page daemon) 1，它然后会很开心地进入休眠状态，因为它毕竟为操作系统释放了一些内存


通过同时执行多个交换过程，我们可以进行一些性能优化。例如，许多系统会把多个要写入的页聚集 (cluster) 或分组 (group) ， 同时写入到交换区间， 从而提高硬盘的效率。这种合并操作减少了硬盘的寻道和旋转开销，从而显著提高了性能。

在所有的讨论中，我们忽略了一些细节。比如，在分页系统中，一般来说面向用户的寄存器内使用的是虚拟地址，类似CR3(指向当前进程页表)这种CPU内部使用寄存器存的是物理地址。**注意，进程上下文切换并不会导致原来的进程页面全换出去，那样效率太低。个人理解：对于Linux，当前进程发生缺页时优先用本进程内使用的非共享内存页进行交换，其它进程的页表此时几乎不会被换出，如果内存不够的话OS会选择一些其它进程kill掉。(具体实现逻辑还得看OS代码)**

## 6 页面置换算法 ##

发生缺页中断时，OS必须在内存中选择一个页面将其换出内存，以便为即将调入的页面腾出空间。如果要换出的页面在内存驻留期间已经被修改过，就必须把它写回磁盘以更新该页面在磁盘上的副本；如果该页面没有被修改过，那么它的磁盘上的副本不需要被回写，直接使用调入的页面覆盖原有页面就行了。

由于内存只包含系统中所有页的子集，因此可以将其视为系统中虚拟内存页的缓存(cache) 。故页面置换算法本质上可以参考缓存替换算法进行设计。

当内存就是被超额请求时，操作系统应该做什么？目前的一些系统采用更严格的方法处理内存过载。例如，当内存超额请求时，某些版本的 Linux 会运行“内存不足的杀手程序(out-of-memory killer) ” 。这个守护进程选择一个内存密集型进程并杀死它，从而以不怎么委婉的方式减少内存。虽然成功地减轻了内存压力，但这种方法可能会遇到问题，例如，如果它杀死 X 服务器，就会导致所有需要显示的应用程序不可用。

## 7小结：硬件和操作系统支持 ##

硬件通过增加一个小的、CPU芯片内的 TLB 作为地址转换的缓存。**TLB管理由硬件负责**，采用全关联映射的方式进行缓存映射。

以前的硬件有**复杂的指令集 (有时称为复杂指令集计算机， Complex-Instruction Set Computer，CISC) ，硬件全权处理 TLB 未命中**。为了做到这一点，硬件必须知道页表在内存中的确切位置(通过页表基址寄存器， page-table base register) ，以及页表的确切格式。发生未命中时， 硬件会“遍历”页表，找到正确的页表项，取出想要的转换映射，用它更新 TLB，并重试该指令。这种“旧”体系结构有硬件管理的 TLB，一个例子是 x86 架构，它采用固定的多级页表(multi-level page table，) ，当前页表由 CR3 寄存器指出。 

更现代的体系结构(例如，MIPS R10k、Sun 公司的 SPARC v9，都是精简指令集计算机， Reduced-Instruction Set Computer， RISC) ， 有所谓的软件管理 TLB (softwaremanaged TLB) 。发生 TLB 未命中时，硬件系统会抛出一个异常，这会暂停当前的指令流，将特权级提升至内核模式，跳转至陷阱处理程序(trap handler) 。接下来你可能已经猜到了，**这类RISC结构计算机中的这个陷阱处理程序是操作系统的一段代码，用于处理 TLB 未命中。**这段代码在运行时，会查找页表中的转换映射，然后用特别的“特权”指令更新 TLB，并从陷阱返回。此时，硬件会重试该指令(导致 TLB 命中) 。

不论在哪种系统中，如果页不存在在内存中，都由操作系统负责处理页错误。操作系统的页错误处理程序(page-fault handler)确定要做什么。**几乎所有的系统都在软件中处理页错误（page-fault）**。即使是完全由硬件管理的 TLB（TLB管理及未命中处理），硬件也信任操作系统来管理这个重要的任务。（主要原因在于：首先，页错误导致的硬盘操作很慢。即使操作系统需要很长时间来处理故障，执行大量的指令，但相比于硬盘操作，这些额外开销是很小的。其次，为了能够处理页故障，硬件必须了解交换空间，如何向硬盘发起 I/O 操作，以及很多它当前所不知道的细节。）

**系统中每个进程都有一个页表，页表位于内存中。**页表的确切结构要么由硬件(旧系统，比如CISC)确定，要么由 OS(现代系统，比如RISC)更灵活地管理。

## 8 总结：内存访问发生了什么 ##

假设有一个硬件管理 TLB 的系统。回想一下内存引用发生了什么。

正在运行的进程生成虚拟内存引用(用于获取指令或访问数据) ，在这种情况下，硬件将其转换为物理地址，再从内存中获取所需数据。 硬件首先从虚拟地址获得 VPN，检查 TLB 是否匹配(TLB 命中) ，如果命中，则获得最终的物理地址并从内存中取回。这希望是常见情形，因为它很快(不需要额外的内存访问) 。  

如果在 TLB 中找不到 VPN(即 TLB 未命中) ，则硬件（或操作系统）在内存中查找页表(使用页表基址寄存器) ，并使用 VPN 查找该页的页表项(PTE)作为索引。如果页有效且存在于物理内存中，则硬件（或操作系统）从 PTE 中获得 PFN，将其插入 TLB，并重试该指令，这次产生 TLB 命中。

当硬件在 PTE 中查找时，可能发现页不在物理内存中（页错误）。在页错误时，操作系统被唤起来处理页错误。一段称为“页错误处理程序(page-fault handler) ”的代码会执行，来处理页错误。

操作系统可以用 PTE 中的某些位来存储硬盘地址， 这些位通常用来存储像页的 PFN 这样的数据。当操作系统接收到页错误时，它会在 PTE 中查找地址，并将请求发送到硬盘，将页读取到内存中。当硬盘 I/O 完成时，操作系统会更新页表，将此页标记为存在，更新页表项(PTE)的PFN 字段以记录新获取页的内存位置，并重试指令。下一次重新访问 TLB 还是未命中，然而这次因为页在内存中，因此会将页表中的地址更新到 TLB 中(也可以在处理页错误时更新 TLB 以避免此步骤) 。最后的重试操作会在 TLB 中找到转换映射，从已转换的内存物理地址，获取所需的数据或指令。 

请注意，当 I/O 在运行时，进程将处于阻塞(blocked)状态。因此，当页错误正常处理时，操作系统可以自由地运行其他可执行的进程。因为 I/O 操作是昂贵的，一个进程进行I/O(页错误)时会执行另一个进程，这种交叠(overlap)是多道程序系统充分利用硬件的一种方式。

回想一下，很重要的是(并且令人惊讶的是) ，这些行为对进程都是透明的。对进程而言，它只是访问自己私有的、连续的虚拟内存。在后台，物理页被放置在物理内存中的任意(非连续)位置，有时它们甚至不在内存中，需要从硬盘取回。虽然我们希望在一般情况下内存访问速度很快，但在某些情况下，它需要多个硬盘操作的时间。像执行单条指令这样简单的事情，在最坏的情况下，可能需要很多毫秒才能完成。
